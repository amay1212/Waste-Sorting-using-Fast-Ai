{
  
    
        "post0": {
            "title": "Waste Sorting using Fast AI v2",
            "content": "This notebook is an extension of Waste Sorter by Collindching and parts of code snippets and code cells have been heavily referred with some tweaks from the above mentioned link. . Primary focus of this notebook to extend the Model&#39;s performance by improving the accuracy and reducing the misclassified error which was noticed earlier. . This notebook is built on fast.ai v2 library. More information can be found here https://course.fast.ai/. Current version of Collindching&#39;s notebook supported fast ai v1 version. So, some tweaks have been done to support that. . Why waste sorting? . Recycling contamination occurs when waste is incorrectly disposed of - like recycling a pizza box with oil on it (compost). Or when waste is correctly disposed of but incorrectly prepared - like recycling unrinsed jam jars. . Contamination is a huge problem in the recycling industry that can be mitigated with automated waste sorting. Just for kicks, I thought I&#39;d try my hand at prototyping an image classifier to classify trash and recyclables - this classifier could have applications in an optical sorting system. . Waste Classifier Model Pipeline . In this project, I&#39;ll try to reduce the misclassification error which was noticed earlier (link mentioned above) . We will follow the same prior steps . Extract data | Investigate on why more misclassification happened in the first place. | Model Data | Predictions on New Images | Comparing our Results to previous version of Notebook | Further enhancements and Research | The below code cell is simply a upgrade step of fastai library, this is done to ensure we have latest fixes in one place. I noticed while plotting top losses of images that there were few empty plots, and this was the quick fix I could find. So, to be on safer side this additional step is performed. . import warnings warnings.filterwarnings(&#39;ignore&#39;) !pip install --upgrade git+https://github.com/fastai/fastai.git . Collecting git+https://github.com/fastai/fastai.git Cloning https://github.com/fastai/fastai.git to /tmp/pip-req-build-s_v1f852 Running command git clone -q https://github.com/fastai/fastai.git /tmp/pip-req-build-s_v1f852 Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (21.1.3) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (21.3) Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (0.0.5) Requirement already satisfied: fastcore&lt;1.4,&gt;=1.3.27 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.3.27) Requirement already satisfied: torchvision&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (0.11.1+cu111) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (3.2.2) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.3.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (2.23.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (3.13) Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.0.0) Requirement already satisfied: pillow&gt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (7.1.2) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.0.2) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.4.1) Requirement already satisfied: spacy&lt;4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (2.2.4) Requirement already satisfied: torch&lt;1.11,&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.10.0+cu111) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress&gt;=0.2.4-&gt;fastai==2.5.4) (1.21.5) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (0.4.1) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (1.1.3) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (1.0.5) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (3.0.6) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (7.4.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (1.0.6) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (0.9.0) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (4.62.3) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (1.0.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai==2.5.4) (57.4.0) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai==2.5.4) (4.11.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai==2.5.4) (3.7.0) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai==2.5.4) (3.10.0.2) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.5.4) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.5.4) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.5.4) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai==2.5.4) (1.24.3) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.5.4) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.5.4) (0.11.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.5.4) (2.8.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai==2.5.4) (3.0.7) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;fastai==2.5.4) (1.15.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;fastai==2.5.4) (2018.9) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai==2.5.4) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai==2.5.4) (1.1.0) . %reload_ext autoreload %autoreload 2 %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; . !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * from fastai.vision.widgets import * from pathlib import Path from glob2 import glob from sklearn.metrics import confusion_matrix import pandas as pd import numpy as np import os import zipfile as zf import shutil import re import seaborn as sns . 1. Extract data . First, we need to extract the contents of &quot;dataset-resized.zip&quot;. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&#34;/content/gdrive&#34;, force_remount=True). . !cp -r gdrive/MyDrive/dataset-resized.zip . files = zf.ZipFile(&quot;dataset-resized.zip&quot;,&#39;r&#39;) files.extractall() files.close() . Once unzipped, the dataset-resized folder has six subfolders: . os.listdir(os.path.join(os.getcwd(),&quot;dataset-resized&quot;)) . [&#39;plastic&#39;, &#39;trash&#39;, &#39;cardboard&#39;, &#39;metal&#39;, &#39;glass&#39;, &#39;paper&#39;, &#39;.DS_Store&#39;] . 2. Organize images into different folders . Now that we&#39;ve extracted the data, I&#39;m going to split images up into train, validation, and test image folders with a 50-25-25 split. First, I&#39;ll define some functions that will help me quickly build it. If you&#39;re not interested in building the data set, you can just run this ignore it. . ?random . f = os.path.join(&#39;dataset-resized&#39;, &#39;plastic&#39;) n = len(os.listdir(f)) k = random.sample(list(range(1,n+1)),int(.5*n)) print(k) . [254, 390, 231, 242, 334, 195, 404, 108, 49, 250, 15, 458, 428, 200, 222, 312, 391, 393, 2, 357, 229, 137, 370, 411, 118, 303, 53, 163, 16, 12, 14, 333, 278, 5, 196, 352, 111, 217, 372, 472, 271, 114, 392, 225, 482, 284, 120, 177, 119, 347, 113, 481, 236, 149, 453, 214, 285, 329, 52, 96, 323, 371, 152, 62, 381, 171, 460, 365, 257, 445, 260, 344, 98, 156, 146, 301, 256, 259, 202, 302, 18, 246, 125, 418, 207, 213, 341, 89, 188, 281, 360, 346, 378, 192, 45, 439, 340, 261, 56, 84, 267, 476, 190, 251, 376, 454, 241, 23, 158, 361, 315, 304, 297, 399, 332, 88, 87, 258, 117, 7, 103, 277, 465, 434, 208, 264, 435, 296, 181, 430, 138, 338, 440, 467, 3, 197, 263, 67, 266, 288, 106, 219, 29, 247, 187, 292, 437, 362, 405, 212, 249, 183, 397, 178, 1, 276, 373, 320, 314, 170, 235, 308, 443, 471, 91, 282, 300, 93, 47, 283, 131, 17, 37, 43, 9, 232, 8, 144, 128, 447, 57, 95, 356, 429, 36, 86, 82, 349, 442, 366, 140, 151, 233, 165, 255, 243, 59, 13, 160, 198, 176, 216, 97, 133, 384, 130, 262, 475, 468, 11, 116, 10, 204, 75, 19, 83, 462, 412, 396, 432, 480, 115, 377, 203, 289, 463, 31, 189, 77, 33, 248, 55, 387, 426, 79, 268, 220, 20, 80, 322, 431] . ## helper functions ## ## splits indices for a folder into train, validation, and test indices with random sampling ## input: folder path ## output: train, valid, and test indices def split_indices(folder,seed1,seed2): n = len(os.listdir(folder)) print(&quot;Folder path:{}&quot;.format(folder)) full_set = list(range(1,n+1)) ## train indices random.seed(seed1) train = random.sample(list(range(1,n+1)),int(.5*n)) ## temp remain = list(set(full_set)-set(train)) ## separate remaining into validation and test random.seed(seed2) valid = random.sample(remain,int(.5*len(remain))) test = list(set(remain)-set(valid)) print(&quot;List of indices n {}. n.{} n.{}&quot;.format(train, valid, test)) return(train,valid,test) ## gets file names for a particular type of trash, given indices ## input: waste category and indices ## output: file names def get_names(waste_type,indices): file_names = [waste_type+str(i)+&quot;.jpg&quot; for i in indices] return(file_names) ## moves group of source files to another folder ## input: list of source files and destination folder ## no output def move_files(source_files,destination_folder): for file in source_files: shutil.move(file,destination_folder) . Next, We will follow the same convention as Imagenet architecture . /data &nbsp;&nbsp;&nbsp;&nbsp; /train &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /cardboard &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /glass &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /metal &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /paper &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /plastic &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /trash &nbsp;&nbsp;&nbsp;&nbsp; /valid &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /cardboard &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /glass &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /metal &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /paper &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /plastic &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /trash &nbsp;&nbsp;&nbsp;&nbsp;/test . Each image file is just the material name and a number (i.e. cardboard1.jpg) . path = Path(os.getcwd()+&quot;/data&quot;) subset = [&#39;train&#39;, &#39;valid&#39;, &#39;test&#39;] [shutil.rmtree(os.path.join(path, sub_folder)) for sub_folder in subset if os.path.exists(os.path.join(path, sub_folder))] . [None, None, None] . ## paths will be train/cardboard, train/glass, etc... subsets = [&#39;train&#39;,&#39;valid&#39;] waste_types = [&#39;cardboard&#39;,&#39;glass&#39;,&#39;metal&#39;,&#39;paper&#39;,&#39;plastic&#39;,&#39;trash&#39;] def verify_source_file(waste_type, dataset_ind): dataset_names = get_names(waste_type, dataset_ind) source_files = [] for name in dataset_names: path = os.path.join(source_folder,name) if not os.path.exists(path): continue source_files.append(path) return source_files ## create destination folders for data subset and waste type for subset in subsets: for waste_type in waste_types: folder = os.path.join(&#39;data&#39;,subset,waste_type) if not os.path.exists(folder): os.makedirs(folder) if not os.path.exists(os.path.join(&#39;data&#39;,&#39;test&#39;)): os.makedirs(os.path.join(&#39;data&#39;,&#39;test&#39;)) ## move files to destination folders for each waste type for waste_type in waste_types: source_folder = os.path.join(&#39;dataset-resized&#39;,waste_type) train_ind, valid_ind, test_ind = split_indices(source_folder,1,1) train_source_files = verify_source_file(waste_type, train_ind) train_dest = &quot;data/train/&quot;+waste_type move_files(train_source_files,train_dest) ## move source files to valid valid_source_files = verify_source_file(waste_type, valid_ind) valid_dest = &quot;data/valid/&quot;+waste_type move_files(valid_source_files,valid_dest) ## move source files to test test_source_files = verify_source_file(waste_type, test_ind) move_files(test_source_files,&quot;data/test&quot;) . Folder path:dataset-resized/cardboard List of indices [69, 292, 392, 33, 131, 61, 254, 390, 231, 242, 334, 195, 108, 49, 250, 15, 200, 222, 312, 2, 357, 229, 137, 370, 118, 303, 53, 163, 16, 12, 14, 333, 278, 5, 196, 352, 111, 217, 388, 271, 114, 225, 397, 284, 120, 177, 119, 347, 113, 236, 149, 374, 214, 285, 329, 52, 96, 323, 152, 62, 171, 257, 366, 260, 98, 156, 146, 301, 256, 259, 202, 302, 18, 246, 125, 207, 213, 89, 188, 281, 192, 45, 362, 261, 56, 84, 267, 372, 190, 251, 375, 241, 23, 158, 304, 297, 316, 88, 87, 258, 117, 7, 103, 277, 324, 383, 208, 264, 358, 181, 354, 138, 299, 3, 197, 263, 67, 266, 106, 219, 29, 247, 187, 336, 393, 212, 249, 183, 327, 178, 1, 170, 235, 365, 379, 91, 93, 47, 399, 17, 37, 43, 9, 232, 8, 144, 128, 402, 283, 205, 160, 48, 326, 75, 331, 262, 41, 66, 136, 244, 44, 169, 70, 166, 276, 76, 378, 180, 83, 342, 122, 30, 332, 80, 99, 306, 391, 204, 396, 287, 28, 65, 291, 265, 54, 338, 367, 210, 6, 58, 380, 102, 38, 10, 185, 42, 115, 294, 130, 174, 110]. .[79, 307, 389, 39, 142, 73, 269, 387, 237, 245, 341, 201, 123, 60, 255, 24, 206, 227, 319, 4, 359, 234, 147, 132, 314, 64, 167, 373, 21, 22, 384, 290, 13, 382, 126, 224, 351, 286, 127, 230, 400, 296, 133, 182, 325, 337, 239, 155, 350, 221, 298, 63, 105, 157, 394, 175, 272, 348, 273, 107, 161, 153, 270, 300, 209, 25, 252, 139, 216, 315, 100, 193, 198, 57, 335, 173, 395, 34, 371, 51, 143, 223, 112, 311, 282, 274, 19, 134, 317, 90, 186, 305, 162, 310, 318, 243, 168, 55, 218, 141, 72] .[11, 20, 26, 27, 31, 32, 35, 36, 40, 46, 50, 59, 68, 71, 74, 77, 78, 81, 82, 85, 86, 92, 94, 95, 97, 101, 104, 109, 116, 121, 124, 129, 135, 140, 145, 148, 150, 151, 154, 159, 164, 165, 172, 176, 179, 184, 189, 191, 194, 199, 203, 211, 215, 220, 226, 228, 233, 238, 240, 248, 253, 268, 275, 279, 280, 288, 289, 293, 295, 308, 309, 313, 320, 321, 322, 328, 330, 339, 340, 343, 344, 345, 346, 349, 353, 355, 356, 360, 361, 363, 364, 368, 369, 376, 377, 381, 385, 386, 398, 401, 403] Folder path:dataset-resized/glass List of indices [69, 292, 434, 411, 392, 33, 131, 61, 254, 390, 231, 242, 334, 195, 404, 108, 49, 250, 15, 458, 428, 200, 222, 312, 391, 393, 2, 357, 229, 137, 370, 492, 118, 303, 53, 163, 16, 12, 14, 333, 278, 5, 452, 196, 352, 111, 217, 372, 477, 271, 114, 491, 225, 487, 284, 120, 177, 119, 347, 113, 486, 236, 149, 476, 214, 429, 285, 329, 52, 96, 323, 371, 152, 62, 381, 171, 465, 365, 257, 449, 260, 344, 98, 156, 146, 301, 256, 259, 202, 302, 18, 246, 125, 421, 207, 213, 341, 89, 188, 281, 360, 346, 378, 192, 45, 443, 340, 261, 56, 84, 267, 407, 190, 251, 376, 459, 241, 23, 158, 361, 315, 304, 297, 384, 332, 88, 87, 258, 117, 7, 103, 277, 396, 438, 208, 264, 439, 296, 181, 493, 138, 338, 363, 472, 3, 197, 263, 67, 266, 288, 106, 219, 29, 247, 187, 494, 441, 418, 408, 212, 249, 183, 400, 178, 1, 276, 364, 320, 314, 170, 235, 308, 447, 463, 91, 282, 300, 93, 47, 283, 489, 17, 37, 43, 9, 232, 8, 144, 128, 355, 57, 95, 359, 433, 36, 86, 82, 375, 446, 369, 140, 151, 233, 165, 255, 243, 59, 13, 160, 198, 176, 216, 97, 133, 387, 130, 262, 480, 473, 11, 116, 10, 204, 75, 19, 83, 467, 415, 414, 436, 265, 485, 115, 380, 203, 389, 385, 31, 153, 65, 248, 55, 435, 394, 79, 373, 220]. .[73, 307, 440, 420, 403, 40, 136, 68, 270, 402, 234, 244, 339, 193, 413, 110, 54, 268, 26, 460, 431, 199, 224, 322, 483, 405, 4, 362, 230, 142, 379, 484, 124, 317, 60, 166, 461, 24, 25, 470, 293, 20, 469, 354, 121, 221, 382, 427, 289, 122, 453, 227, 478, 298, 126, 175, 401, 351, 409, 238, 154, 426, 215, 299, 336, 58, 101, 330, 155, 479, 172, 273, 416, 274, 349, 102, 159, 148, 316, 272, 353, 201, 345, 27, 252, 132, 209, 468, 94, 185, 295, 189, 50, 451, 275, 63, 85, 286, 424, 186, 269, 412, 471, 32, 161, 313, 92, 475, 356, 437, 21, 107, 398, 367, 279, 399, 180, 388, 444, 6, 194, 442, 223, 211] .[22, 28, 30, 34, 35, 38, 39, 41, 42, 44, 46, 48, 51, 64, 66, 70, 71, 72, 74, 76, 77, 78, 80, 81, 90, 99, 100, 104, 105, 109, 112, 123, 127, 129, 134, 135, 139, 141, 143, 145, 147, 150, 157, 162, 164, 167, 168, 169, 173, 174, 179, 182, 184, 191, 205, 206, 210, 218, 226, 228, 237, 239, 240, 245, 253, 280, 287, 290, 291, 294, 305, 306, 309, 310, 311, 318, 319, 321, 324, 325, 326, 327, 328, 331, 335, 337, 342, 343, 348, 350, 358, 366, 368, 374, 377, 383, 386, 395, 397, 406, 410, 417, 419, 422, 423, 425, 430, 432, 445, 448, 450, 454, 455, 456, 457, 462, 464, 466, 474, 481, 482, 488, 490, 495] Folder path:dataset-resized/metal List of indices [69, 292, 392, 33, 131, 61, 254, 390, 231, 242, 334, 195, 108, 49, 250, 15, 200, 222, 312, 391, 2, 357, 229, 137, 370, 118, 303, 53, 163, 16, 12, 14, 333, 278, 5, 196, 352, 111, 217, 395, 271, 114, 225, 404, 284, 120, 177, 119, 347, 113, 236, 149, 380, 214, 285, 329, 52, 96, 323, 152, 62, 171, 257, 372, 260, 344, 98, 156, 146, 301, 256, 259, 202, 302, 18, 246, 125, 207, 213, 89, 188, 281, 192, 45, 368, 261, 56, 84, 267, 338, 190, 251, 381, 241, 23, 158, 304, 297, 321, 88, 87, 258, 117, 7, 103, 277, 355, 363, 208, 264, 364, 296, 181, 360, 138, 314, 3, 197, 263, 67, 266, 288, 106, 219, 29, 247, 187, 306, 339, 212, 249, 183, 332, 178, 1, 170, 235, 371, 385, 91, 93, 47, 406, 17, 37, 43, 9, 232, 8, 144, 128, 299, 57, 95, 300, 75, 336, 265, 41, 66, 136, 244, 44, 169, 70, 166, 279, 76, 308, 180, 83, 346, 122, 30, 307, 80, 99, 311, 398, 204, 397, 291, 28, 65, 366, 268, 54, 343, 373, 210, 6, 58, 376, 102, 38, 10, 185, 42, 115, 298, 130, 174, 110, 140, 309]. .[79, 313, 399, 39, 145, 73, 269, 396, 237, 245, 348, 203, 124, 60, 255, 24, 206, 227, 326, 4, 367, 234, 150, 133, 320, 64, 168, 387, 21, 22, 401, 289, 13, 394, 127, 224, 365, 283, 129, 230, 402, 294, 134, 184, 340, 359, 239, 159, 362, 221, 295, 63, 107, 160, 403, 176, 272, 351, 273, 109, 162, 155, 270, 315, 209, 25, 252, 141, 216, 383, 101, 194, 199, 55, 345, 68, 94, 293, 322, 287, 280, 19, 135, 327, 90, 189, 316, 164, 374, 328, 116, 172, 51, 274, 143, 72, 11, 205, 324, 151, 153, 226] .[20, 26, 27, 31, 32, 34, 35, 36, 40, 46, 48, 50, 59, 71, 74, 77, 78, 81, 82, 85, 86, 92, 97, 100, 104, 105, 112, 121, 123, 126, 132, 139, 142, 147, 148, 154, 157, 161, 165, 167, 173, 175, 179, 182, 186, 191, 193, 198, 201, 211, 215, 218, 220, 223, 228, 233, 238, 240, 243, 248, 253, 262, 275, 276, 282, 286, 290, 305, 310, 317, 318, 319, 325, 330, 331, 335, 337, 341, 342, 349, 350, 353, 354, 356, 358, 361, 369, 375, 377, 378, 379, 382, 384, 386, 388, 389, 393, 400, 405, 407, 408, 409, 410] Folder path:dataset-resized/paper List of indices [138, 583, 65, 262, 121, 508, 461, 484, 389, 215, 97, 500, 30, 400, 444, 3, 457, 273, 235, 105, 326, 32, 23, 27, 555, 10, 391, 222, 433, 582, 541, 228, 449, 589, 239, 354, 237, 225, 471, 297, 572, 427, 103, 191, 304, 124, 341, 513, 566, 520, 195, 311, 291, 512, 518, 403, 36, 492, 249, 414, 425, 178, 376, 384, 89, 450, 521, 111, 168, 539, 380, 502, 31, 481, 45, 316, 404, 175, 173, 515, 233, 13, 205, 277, 472, 441, 281, 119, 208, 264, 177, 488, 434, 296, 181, 236, 466, 594, 338, 561, 312, 491, 374, 579, 197, 402, 439, 421, 454, 528, 524, 263, 415, 67, 266, 399, 288, 106, 219, 29, 247, 446, 187, 292, 284, 552, 259, 212, 536, 417, 183, 213, 533, 1, 276, 511, 320, 525, 314, 170, 576, 308, 15, 412, 118, 574, 91, 282, 300, 93, 509, 47, 409, 283, 442, 436, 419, 131, 17, 431, 345, 37, 43, 9, 232, 8, 387, 428, 144, 128, 497, 57, 440, 458, 95, 504, 149, 538, 86, 82, 437, 271, 87, 337, 140, 332, 365, 151, 514, 360, 165, 255, 243, 59, 547, 160, 198, 176, 216, 584, 133, 56, 130, 537, 591, 108, 543, 567, 11, 116, 569, 204, 75, 19, 369, 83, 229, 361, 260, 348, 476, 279, 113, 323, 356, 265, 231, 115, 269, 333, 16, 203, 346, 295, 540, 496, 324, 568, 522, 153, 592, 109, 25, 157, 546, 40, 159, 355, 482, 214, 290, 392, 485, 5, 478, 20, 303, 112, 556, 499, 88, 319, 261, 339, 194, 469, 462, 51, 477, 294, 553, 359, 100, 253, 54, 200, 152, 468, 256, 435, 167, 206, 145, 531, 81, 329, 526, 289, 70, 174, 220, 110, 137, 50, 544, 495, 438]. .[78, 501, 42, 192, 573, 418, 381, 397, 328, 148, 58, 410, 532, 335, 368, 4, 378, 207, 166, 62, 258, 21, 529, 530, 7, 330, 155, 363, 486, 161, 372, 507, 171, 293, 169, 158, 388, 234, 464, 358, 61, 127, 257, 310, 367, 85, 549, 321, 102, 379, 309, 306, 189, 395, 150, 190, 357, 385, 280, 489, 593, 586, 240, 373, 188, 455, 494, 136, 386, 581, 12, 180, 72, 487, 344, 142, 147, 564, 559, 125, 217, 301, 334, 565, 317, 422, 542, 432, 278, 505, 548, 483, 558, 199, 407, 126, 185, 315, 523, 172, 426, 90, 302, 250, 241, 429, 342, 270, 53, 396, 401, 68, 6, 423, 210, 382, 69, 480, 193, 114, 230, 120, 473, 510, 275, 305, 246, 479, 134, 299, 550, 196, 223, 470, 154, 453, 182, 364, 226, 218, 307, 498, 146, 184, 122, 416, 298, 2] .[516, 517, 519, 14, 527, 18, 22, 535, 534, 24, 26, 28, 33, 34, 35, 545, 38, 39, 551, 41, 554, 44, 557, 46, 48, 49, 560, 562, 52, 563, 55, 570, 571, 60, 63, 64, 575, 66, 578, 577, 580, 71, 73, 74, 585, 76, 77, 587, 79, 80, 590, 588, 84, 92, 94, 96, 98, 99, 101, 104, 107, 117, 123, 129, 132, 135, 139, 141, 143, 156, 162, 163, 164, 179, 186, 201, 202, 209, 211, 221, 224, 227, 238, 242, 244, 245, 248, 251, 252, 254, 267, 268, 272, 274, 285, 286, 287, 313, 318, 322, 325, 327, 331, 336, 340, 343, 347, 349, 350, 351, 352, 353, 362, 366, 370, 371, 375, 377, 383, 390, 393, 394, 398, 405, 406, 408, 411, 413, 420, 424, 430, 443, 445, 447, 448, 451, 452, 456, 459, 460, 463, 465, 467, 474, 475, 490, 493, 503, 506] Folder path:dataset-resized/plastic List of indices [69, 292, 434, 411, 392, 33, 131, 61, 254, 390, 231, 242, 334, 195, 404, 108, 49, 250, 15, 458, 428, 200, 222, 312, 391, 393, 2, 357, 229, 137, 370, 479, 118, 303, 53, 163, 16, 12, 14, 333, 278, 5, 196, 352, 111, 217, 372, 464, 271, 114, 478, 225, 474, 284, 120, 177, 119, 347, 113, 473, 236, 149, 445, 214, 285, 329, 52, 96, 323, 371, 152, 62, 381, 171, 452, 365, 257, 437, 260, 344, 98, 156, 146, 301, 256, 259, 202, 302, 18, 246, 125, 410, 207, 213, 341, 89, 188, 281, 360, 346, 378, 192, 45, 431, 340, 261, 56, 84, 267, 396, 190, 251, 446, 241, 23, 158, 361, 315, 304, 297, 373, 332, 88, 87, 258, 117, 7, 103, 277, 385, 426, 208, 264, 427, 296, 181, 422, 138, 338, 353, 459, 3, 197, 263, 67, 266, 288, 106, 219, 29, 247, 187, 481, 429, 355, 397, 212, 249, 183, 389, 178, 1, 276, 354, 314, 170, 235, 308, 435, 450, 91, 282, 300, 93, 47, 283, 476, 17, 37, 43, 9, 232, 8, 144, 128, 345, 57, 95, 349, 421, 36, 86, 82, 306, 480, 359, 140, 151, 233, 165, 255, 243, 59, 13, 160, 198, 176, 216, 97, 133, 376, 130, 262, 467, 460, 11, 116, 10, 204, 75, 19, 83, 454, 470, 424, 472, 115, 135, 167, 310, 102, 173, 148, 206, 377, 169, 162, 110, 408, 189, 77]. .[70, 313, 439, 417, 403, 39, 136, 64, 273, 402, 240, 253, 348, 205, 413, 107, 51, 270, 26, 433, 210, 230, 326, 469, 405, 4, 374, 239, 142, 384, 471, 123, 321, 55, 168, 447, 24, 25, 456, 295, 20, 455, 367, 112, 227, 386, 425, 291, 121, 237, 463, 305, 124, 184, 394, 364, 400, 245, 154, 416, 224, 307, 342, 54, 94, 336, 155, 465, 179, 275, 475, 279, 99, 159, 150, 320, 274, 356, 211, 343, 27, 268, 129, 220, 379, 85, 194, 299, 201, 48, 399, 280, 58, 79, 289, 337, 199, 272, 462, 457, 31, 161, 477, 81, 294, 362, 423, 21, 104, 387, 328, 388, 186, 383, 244, 482, 175, 145, 350, 325] .[6, 22, 28, 30, 32, 34, 35, 38, 40, 41, 42, 44, 46, 50, 60, 63, 65, 66, 68, 71, 72, 73, 74, 76, 78, 80, 90, 92, 100, 101, 105, 109, 122, 126, 127, 132, 134, 139, 141, 143, 147, 153, 157, 164, 166, 172, 174, 180, 182, 185, 191, 193, 203, 209, 215, 218, 221, 223, 226, 228, 234, 238, 248, 252, 265, 269, 286, 287, 290, 293, 298, 309, 311, 316, 317, 318, 319, 322, 324, 327, 330, 331, 335, 339, 351, 358, 363, 366, 368, 369, 375, 380, 382, 395, 398, 401, 406, 407, 409, 412, 414, 415, 418, 419, 420, 430, 432, 436, 438, 440, 441, 442, 443, 444, 448, 449, 451, 453, 461, 466, 468] Folder path:dataset-resized/trash List of indices [35, 17, 66, 31, 127, 116, 121, 98, 54, 25, 63, 4, 115, 107, 50, 56, 78, 130, 99, 1, 90, 58, 137, 93, 103, 30, 76, 14, 41, 126, 3, 108, 84, 70, 2, 49, 88, 28, 55, 114, 106, 68, 29, 57, 64, 71, 112, 45, 91, 87, 95, 59, 38, 124, 129, 72, 13, 24, 85, 16, 43, 65, 119, 111, 128, 39, 37, 120]. .[20, 11, 48, 18, 113, 101, 46, 73, 33, 86, 135, 9, 47, 132, 94, 105, 40, 62, 82, 83, 131, 77, 42, 125, 136, 61, 117, 23, 97, 69, 67, 52, 104, 96] .[5, 6, 7, 8, 133, 10, 134, 12, 15, 19, 21, 22, 26, 27, 32, 34, 36, 44, 51, 53, 60, 74, 75, 79, 80, 81, 89, 92, 100, 102, 109, 110, 118, 122, 123] . I set the seed for both random samples to be 1 for reproducibility. Now that the data&#39;s organized, we can get to model training. . Important things before we go any further, fast.ai uses Path extensively for handling image path which makes more convenient to not worry about the directory where the images have to be loaded from. . In short, path is just the working directory where temporary files/models will be saved. . Path is used extensively in fastai reference. . path = Path(os.getcwd())/&quot;data&quot; path . Path(&#39;/content/data&#39;) . print(&quot;Plastic {}&quot;.format(len(os.listdir(str(path)+&#39;/train/plastic&#39;)))) print(&quot;Plastic {}&quot;.format(len(os.listdir(str(path)+&#39;/valid/plastic&#39;)))) print(&quot;Glass tr {}&quot;.format(len(os.listdir(str(path)+&#39;/train/glass&#39;)))) print(&quot;Glass Vl {}&quot;.format(len(os.listdir(str(path)+&#39;/valid/glass&#39;)))) . Plastic 241 Plastic 120 Glass tr 242 Glass Vl 124 . It seems like we are getting a normal split between train and valid set for glass images. Will look further to improve classification problem by augmentation and so on.. . os.listdir(str(path) + &#39;/train/glass&#39;) . [&#39;glass151.jpg&#39;, &#39;glass49.jpg&#39;, &#39;glass106.jpg&#39;, &#39;glass246.jpg&#39;, &#39;glass446.jpg&#39;, &#39;glass359.jpg&#39;, &#39;glass163.jpg&#39;, &#39;glass378.jpg&#39;, &#39;glass1.jpg&#39;, &#39;glass36.jpg&#39;, &#39;glass297.jpg&#39;, &#39;glass117.jpg&#39;, &#39;glass178.jpg&#39;, &#39;glass361.jpg&#39;, &#39;glass82.jpg&#39;, &#39;glass146.jpg&#39;, &#39;glass188.jpg&#39;, &#39;glass340.jpg&#39;, &#39;glass396.jpg&#39;, &#39;glass428.jpg&#39;, &#39;glass15.jpg&#39;, &#39;glass233.jpg&#39;, &#39;glass271.jpg&#39;, &#39;glass276.jpg&#39;, &#39;glass83.jpg&#39;, &#39;glass380.jpg&#39;, &#39;glass494.jpg&#39;, &#39;glass312.jpg&#39;, &#39;glass256.jpg&#39;, &#39;glass87.jpg&#39;, &#39;glass360.jpg&#39;, &#39;glass308.jpg&#39;, &#39;glass93.jpg&#39;, &#39;glass389.jpg&#39;, &#39;glass487.jpg&#39;, &#39;glass258.jpg&#39;, &#39;glass152.jpg&#39;, &#39;glass14.jpg&#39;, &#39;glass385.jpg&#39;, &#39;glass370.jpg&#39;, &#39;glass404.jpg&#39;, &#39;glass52.jpg&#39;, &#39;glass59.jpg&#39;, &#39;glass18.jpg&#39;, &#39;glass67.jpg&#39;, &#39;glass443.jpg&#39;, &#39;glass341.jpg&#39;, &#39;glass75.jpg&#39;, &#39;glass393.jpg&#39;, &#39;glass363.jpg&#39;, &#39;glass434.jpg&#39;, &#39;glass381.jpg&#39;, &#39;glass447.jpg&#39;, &#39;glass86.jpg&#39;, &#39;glass229.jpg&#39;, &#39;glass433.jpg&#39;, &#39;glass91.jpg&#39;, &#39;glass61.jpg&#39;, &#39;glass255.jpg&#39;, &#39;glass103.jpg&#39;, &#39;glass347.jpg&#39;, &#39;glass400.jpg&#39;, &#39;glass394.jpg&#39;, &#39;glass56.jpg&#39;, &#39;glass449.jpg&#39;, &#39;glass247.jpg&#39;, &#39;glass301.jpg&#39;, &#39;glass365.jpg&#39;, &#39;glass197.jpg&#39;, &#39;glass314.jpg&#39;, &#39;glass7.jpg&#39;, &#39;glass196.jpg&#39;, &#39;glass128.jpg&#39;, &#39;glass390.jpg&#39;, &#39;glass200.jpg&#39;, &#39;glass69.jpg&#39;, &#39;glass3.jpg&#39;, &#39;glass111.jpg&#39;, &#39;glass19.jpg&#39;, &#39;glass372.jpg&#39;, &#39;glass235.jpg&#39;, &#39;glass257.jpg&#39;, &#39;glass241.jpg&#39;, &#39;glass212.jpg&#39;, &#39;glass391.jpg&#39;, &#39;glass467.jpg&#39;, &#39;glass130.jpg&#39;, &#39;glass17.jpg&#39;, &#39;glass477.jpg&#39;, &#39;glass296.jpg&#39;, &#39;glass37.jpg&#39;, &#39;glass84.jpg&#39;, &#39;glass97.jpg&#39;, &#39;glass281.jpg&#39;, &#39;glass282.jpg&#39;, &#39;glass5.jpg&#39;, &#39;glass225.jpg&#39;, &#39;glass57.jpg&#39;, &#39;glass114.jpg&#39;, &#39;glass486.jpg&#39;, &#39;glass302.jpg&#39;, &#39;glass476.jpg&#39;, &#39;glass421.jpg&#39;, &#39;glass261.jpg&#39;, &#39;glass95.jpg&#39;, &#39;glass438.jpg&#39;, &#39;glass415.jpg&#39;, &#39;glass492.jpg&#39;, &#39;glass266.jpg&#39;, &#39;glass140.jpg&#39;, &#39;glass463.jpg&#39;, &#39;glass480.jpg&#39;, &#39;glass170.jpg&#39;, &#39;glass11.jpg&#39;, &#39;glass202.jpg&#39;, &#39;glass88.jpg&#39;, &#39;glass118.jpg&#39;, &#39;glass250.jpg&#39;, &#39;glass371.jpg&#39;, &#39;glass436.jpg&#39;, &#39;glass459.jpg&#39;, &#39;glass323.jpg&#39;, &#39;glass285.jpg&#39;, &#39;glass12.jpg&#39;, &#39;glass387.jpg&#39;, &#39;glass418.jpg&#39;, &#39;glass260.jpg&#39;, &#39;glass357.jpg&#39;, &#39;glass355.jpg&#39;, &#39;glass352.jpg&#39;, &#39;glass113.jpg&#39;, &#39;glass204.jpg&#39;, &#39;glass452.jpg&#39;, &#39;glass369.jpg&#39;, &#39;glass300.jpg&#39;, &#39;glass485.jpg&#39;, &#39;glass491.jpg&#39;, &#39;glass315.jpg&#39;, &#39;glass125.jpg&#39;, &#39;glass47.jpg&#39;, &#39;glass181.jpg&#39;, &#39;glass473.jpg&#39;, &#39;glass216.jpg&#39;, &#39;glass414.jpg&#39;, &#39;glass16.jpg&#39;, &#39;glass408.jpg&#39;, &#39;glass292.jpg&#39;, &#39;glass303.jpg&#39;, &#39;glass89.jpg&#39;, &#39;glass133.jpg&#39;, &#39;glass65.jpg&#39;, &#39;glass119.jpg&#39;, &#39;glass264.jpg&#39;, &#39;glass373.jpg&#39;, &#39;glass153.jpg&#39;, &#39;glass45.jpg&#39;, &#39;glass278.jpg&#39;, &#39;glass108.jpg&#39;, &#39;glass267.jpg&#39;, &#39;glass219.jpg&#39;, &#39;glass156.jpg&#39;, &#39;glass183.jpg&#39;, &#39;glass23.jpg&#39;, &#39;glass376.jpg&#39;, &#39;glass165.jpg&#39;, &#39;glass288.jpg&#39;, &#39;glass131.jpg&#39;, &#39;glass203.jpg&#39;, &#39;glass208.jpg&#39;, &#39;glass242.jpg&#39;, &#39;glass320.jpg&#39;, &#39;glass10.jpg&#39;, &#39;glass304.jpg&#39;, &#39;glass222.jpg&#39;, &#39;glass411.jpg&#39;, &#39;glass220.jpg&#39;, &#39;glass283.jpg&#39;, &#39;glass160.jpg&#39;, &#39;glass33.jpg&#39;, &#39;glass263.jpg&#39;, &#39;glass489.jpg&#39;, &#39;glass429.jpg&#39;, &#39;glass31.jpg&#39;, &#39;glass8.jpg&#39;, &#39;glass79.jpg&#39;, &#39;glass435.jpg&#39;, &#39;glass344.jpg&#39;, &#39;glass334.jpg&#39;, &#39;glass493.jpg&#39;, &#39;glass259.jpg&#39;, &#39;glass177.jpg&#39;, &#39;glass375.jpg&#39;, &#39;glass262.jpg&#39;, &#39;glass243.jpg&#39;, &#39;glass392.jpg&#39;, &#39;glass144.jpg&#39;, &#39;glass137.jpg&#39;, &#39;glass458.jpg&#39;, &#39;glass407.jpg&#39;, &#39;glass116.jpg&#39;, &#39;glass62.jpg&#39;, &#39;glass43.jpg&#39;, &#39;glass55.jpg&#39;, &#39;glass329.jpg&#39;, &#39;glass53.jpg&#39;, &#39;glass158.jpg&#39;, &#39;glass441.jpg&#39;, &#39;glass251.jpg&#39;, &#39;glass120.jpg&#39;, &#39;glass198.jpg&#39;, &#39;glass439.jpg&#39;, &#39;glass346.jpg&#39;, &#39;glass277.jpg&#39;, &#39;glass207.jpg&#39;, &#39;glass2.jpg&#39;, &#39;glass333.jpg&#39;, &#39;glass115.jpg&#39;, &#39;glass232.jpg&#39;, &#39;glass213.jpg&#39;, &#39;glass249.jpg&#39;, &#39;glass332.jpg&#39;, &#39;glass171.jpg&#39;, &#39;glass465.jpg&#39;, &#39;glass214.jpg&#39;, &#39;glass384.jpg&#39;, &#39;glass9.jpg&#39;, &#39;glass217.jpg&#39;, &#39;glass231.jpg&#39;, &#39;glass472.jpg&#39;, &#39;glass138.jpg&#39;, &#39;glass265.jpg&#39;, &#39;glass149.jpg&#39;, &#39;glass284.jpg&#39;, &#39;glass96.jpg&#39;, &#39;glass13.jpg&#39;, &#39;glass236.jpg&#39;, &#39;glass187.jpg&#39;, &#39;glass98.jpg&#39;, &#39;glass29.jpg&#39;, &#39;glass364.jpg&#39;, &#39;glass195.jpg&#39;, &#39;glass338.jpg&#39;] . tr_glass_imgs = os.listdir(str(path)+&#39;/train/glass&#39;) img = Image.open(str(path)+&#39;/train/glass/&#39;+tr_glass_imgs[3]) . In the previous version of this notebook, glass was more misclassified as metal or plastic. . Investigating On Why Glass is more misclassified than Plastic and Metal . glass_imgs = (path/&#39;train/glass&#39;).ls() im = Image.open(glass_imgs[1]) # converting the first image to tensors... first_glass = tensor(im) print(first_glass[1:4, 4:10]) print(first_glass.shape) . tensor([[[230, 210, 186], [229, 209, 185], [229, 209, 185], [229, 209, 185], [228, 208, 184], [228, 208, 184]], [[229, 209, 185], [229, 209, 185], [229, 209, 185], [229, 209, 185], [228, 208, 184], [228, 208, 184]], [[229, 209, 185], [229, 209, 185], [228, 208, 184], [228, 208, 184], [228, 208, 184], [228, 208, 184]]], dtype=torch.uint8) torch.Size([384, 512, 3]) . We have converted our first image of glass to tensor.... . Next steps: . We calculate the mean of the sample glass image and see if we can get closer image of tensor of plastic as well as metal as they were the most misclassified items for our waste classifier . a = torch.randn(20, 4, 4) print(a) random.seed(1) mean_a = a.mean(0) print(mean_a.shape) . tensor([[[ 1.9041, -0.6623, -0.0740, -1.8308], [ 0.0620, -0.2726, 1.5847, -2.0998], [-1.8451, -0.5164, -0.8150, 0.2383], [-0.0154, -1.3963, -0.2346, 0.6368]], [[-0.4682, 0.7713, -1.9177, 0.5771], [ 1.3979, 0.8343, 0.2862, 0.4237], [ 1.6290, -0.2072, -0.1179, -0.9172], [-0.6613, -1.1318, 0.1150, 2.8163]], [[-0.8620, -0.8489, -0.4204, 0.7048], [ 0.1674, -0.9637, -1.4947, 0.9189], [-0.3080, -3.3565, 1.1957, -0.8564], [ 1.2134, -0.4157, 0.0896, -0.1226]], [[-0.5561, 1.2022, -0.3723, 0.9290], [ 1.0448, 0.6112, -1.1486, -1.2055], [-0.5671, -0.7027, -1.4796, 1.7317], [-0.5427, 0.8289, -1.6866, -1.9065]], [[ 0.7942, -0.7050, -1.6597, 1.6873], [ 1.5678, -0.0256, 0.2248, 1.2104], [-1.0935, -0.5420, -0.2103, -2.3326], [ 0.3916, 0.2181, -2.0667, -0.0830]], [[ 1.0960, 0.4500, 0.4413, 0.5867], [-1.6896, -0.7949, 0.0723, 0.4627], [-0.6459, -1.1081, -0.6236, 0.8584], [-0.3111, 0.1923, 0.4706, -1.2359]], [[-2.0230, 0.8408, 0.7278, -0.4549], [-2.4891, -0.0443, 0.7275, -2.5718], [-1.6148, -0.9621, 0.8803, 0.2965], [-0.5933, -0.5933, -0.6970, -0.8499]], [[-0.4422, 0.0171, -2.3430, 0.7095], [-1.2889, 1.5792, 1.1738, 0.6752], [-1.3570, 0.1963, -0.9830, 1.6627], [ 0.6076, 0.0658, 0.3313, -0.1094]], [[-0.0680, -0.5092, 1.9973, -0.2931], [-0.8382, -1.0782, 0.4462, -0.0646], [-0.5631, 1.4206, 1.7943, 1.8898], [ 0.3165, 0.2293, 0.7299, -1.4330]], [[ 1.2045, 0.6211, -1.1297, 0.7161], [ 2.2381, 0.3150, 1.0000, 2.0494], [-0.1351, 1.8014, 0.5025, -0.0968], [-1.1127, -0.5012, -0.8887, 0.2562]], [[ 0.2549, -0.7717, 0.4916, 1.4921], [-0.7116, -0.1897, -1.6547, -0.8994], [ 0.1157, 0.8570, 0.7750, -1.0252], [ 0.4516, 1.6063, -0.4316, -1.5365]], [[ 0.0418, -0.4215, -0.0071, 0.3110], [ 0.2383, -1.7163, -0.3315, 2.7632], [ 0.6609, 0.0607, -0.9238, -0.9787], [ 0.6828, -1.0436, -0.6931, 0.1954]], [[-0.3110, 0.2085, 0.4414, -0.5819], [ 1.5436, -1.5502, -0.8435, -0.7709], [-0.7234, -0.8946, -0.8756, -0.9477], [ 0.0690, -2.8165, 0.9283, -0.6112]], [[-0.1419, -0.1804, 0.4074, -0.2060], [ 1.1673, -0.3224, 0.1169, -1.5055], [ 0.4926, 1.4967, -0.1806, 1.6904], [-0.3416, 0.1862, 0.4881, 0.3701]], [[-0.2108, 0.2191, -1.1873, -1.0986], [ 1.4215, -0.3173, 0.0849, -0.7219], [-0.3716, -0.9612, -0.3766, -0.9942], [ 0.6933, 0.8511, 1.3226, -0.3613]], [[ 0.1691, -1.3188, 0.9293, -1.1307], [ 0.9253, -1.1454, -0.6973, -0.0113], [-1.9940, -0.9999, -1.1215, -0.8117], [-0.6660, 0.9826, 1.1000, 1.2608]], [[ 1.2468, -0.1739, 0.6360, 0.4991], [-0.0972, 1.2482, 0.7075, 2.0349], [ 1.3595, -0.5676, 1.0609, -0.4931], [ 0.8091, 0.3110, -0.4240, -0.5833]], [[ 1.5777, -1.2872, -0.5809, -0.3320], [ 1.2174, -2.0281, -0.2087, -0.7707], [ 1.2203, 0.3608, -2.1707, 0.9702], [ 1.6480, -0.1294, 0.3269, 1.0901]], [[-0.0916, 0.2605, 0.1513, 0.3333], [ 0.8916, -1.0663, -0.6575, -0.8669], [-0.4498, -0.8968, 1.9549, 0.6996], [ 0.6076, 0.1456, 1.4862, 0.0192]], [[-0.2001, 0.9258, 0.2802, -0.7186], [ 0.1723, -1.8113, 0.8975, -1.8717], [-0.7931, -0.5257, -0.3883, -0.5874], [ 0.6360, -0.6921, -0.7885, 1.1872]]]) torch.Size([4, 4]) . The above code explains how 20 random matrices of 4X4 are stacked up and along the first axis we take the mean which is like all 20 (4 X 4) matrices are computed along X and Y axis and we get a mean random matrix of (4X4). . We will use similar process for concatenating the matrices of 384 X 512 X 3 images to the folder length. For example if training set has 240 images of 384 X 512 X 3 channels, this will be combined or stacked up to shape (240, 384, 512, 3) image size. Hence, we can now compute the average pixels of all these 240 images to form one single image formed by mean of all 240 images and that can be compared to a new image with size (384X512X3). This is the base line model which might not give good results comparatively to the state of the art transfer learning models which are used later in the notebook cells. . glass_tensors = [tensor(Image.open(g_img)) for g_img in glass_imgs] print(len(glass_tensors)) plastic_imgs = (path/&#39;train/plastic&#39;).ls() plastic_tensors = [tensor(Image.open(g_img)) for g_img in plastic_imgs] print(len(plastic_tensors)) metal_imgs = (path/&#39;train/metal&#39;).ls() metal_tensors = [tensor(Image.open(g_img)) for g_img in metal_imgs] print(len(metal_tensors)) # stacking up all the images of glass, plastic and metal... glass_stack = torch.stack(glass_tensors).float()/255 print(glass_stack.shape, glass_stack.ndim) plastic_stack = torch.stack(plastic_tensors).float()/255 print(plastic_stack.shape, plastic_stack.ndim) metal_stack = torch.stack(metal_tensors).float()/255 print(metal_stack.shape, metal_stack.ndim) mean_glass_tensor = glass_stack.mean(0) print(&quot;mean shape of glass tensor ==&gt;&quot;, mean_glass_tensor.shape) mean_metal_tensor = metal_stack.mean(0) show_image(mean_metal_tensor) show_image(mean_glass_tensor) . 242 241 205 torch.Size([242, 384, 512, 3]) 4 torch.Size([241, 384, 512, 3]) 4 torch.Size([205, 384, 512, 3]) 4 mean shape of glass tensor ==&gt; torch.Size([384, 512, 3]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff696e7e0d0&gt; . mean_plastic_tensor = plastic_stack.mean(0) show_image(mean_plastic_tensor) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff699b4d690&gt; . diff1 = (glass_stack[1] - mean_glass_tensor).abs().mean() print(diff1) diff2 = (glass_stack[1] - mean_plastic_tensor).abs().mean() print(diff2) diff3 = (glass_stack[1] - mean_metal_tensor).abs().mean() print(diff3) # Clearly the lowest distance can be found for the glass tensors, then plastic and metal follow along in ranking.. # Hence this ia a baseline model to classify this as a glass tensor . tensor(0.0943) tensor(0.0994) tensor(0.1026) . def glass_distance(a,b): return (a-b).abs().mean((-1, -2)) def is_glass(x): return glass_distance(x,mean_glass_tensor[:, :, 0]) &lt; glass_distance(x,mean_plastic_tensor[:, :, 0]) . is_glass(plastic_stack[-1][:, :, 0]), is_glass(plastic_stack[-1][:, :, 0]).float() . (tensor(False), tensor(0.)) . valid_path = (path/&#39;valid/glass&#39;).ls() print(len(valid_path)) valid_path = (path/&#39;valid/plastic&#39;).ls() print(len(valid_path)) valid_path = (path/&#39;train/glass&#39;).ls() print(len(valid_path)) valid_path = (path/&#39;train/plastic&#39;).ls() print(len(valid_path)) ####### valid_path = (path/&#39;valid/plastic&#39;).ls() plastic_imgs = [Image.open(img) for img in valid_path] plastic_valid_tens = torch.stack([tensor(Image.open(img)) for img in valid_path]) plastic_valid_tens = plastic_valid_tens.float()/255 valid_plastic_tens = plastic_valid_tens[:, :, :, 0] trues = [i for i in is_glass(valid_plastic_tens) if i == True] false = [i for i in is_glass(valid_plastic_tens) if i == False] total = trues + false all = len(total) print(len(trues), len(false)) . 124 120 242 241 68 52 . Accuracy of our initial base line model constructed on the fact of comparing average image pixels with new glass or metal image. . valid_path = (path/&#39;valid/glass&#39;).ls() glass_imgs = [Image.open(img) for img in valid_path] #print(valid_path) glass_valid_tens = torch.stack([tensor(Image.open(img)) for img in valid_path]) glass_valid_tensors = glass_valid_tens.float()/255 valid_glass_tens = glass_valid_tensors[:, :, :, 0] accuracy_glass = is_glass(valid_glass_tens).float() .mean() accuracy_plastic = (1 - is_glass(valid_plastic_tens).float()).mean() # simple accuracy formula to compute... print(accuracy_glass,accuracy_plastic,(accuracy_plastic+accuracy_glass)/2) . A simplistic implementation of creating a base line model to see why model can get confuse between metal, glass images because the distances of these tensors are very close to each other. In simpler terms these are most correlated ones.. . Let&#39;s try to put our model into implementation by first apply augmented transforms to it and then putting these details into our model. Here are some details about the function . doc(aug_transforms) . aug_transforms[source] . aug_transforms(mult=1.0, do_flip=True, flip_vert=False, max_rotate=10.0, min_zoom=1.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None, size=None, mode=&#39;bilinear&#39;, pad_mode=&#39;reflection&#39;, align_corners=True, batch=False, min_scale=1.0) . Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms. . Type Default . mult | float | 1.0 | . do_flip | bool | True | . flip_vert | bool | False | . max_rotate | float | 10.0 | . min_zoom | float | 1.0 | . max_zoom | float | 1.1 | . max_lighting | float | 0.2 | . max_warp | float | 0.2 | . p_affine | float | 0.75 | . p_lighting | float | 0.75 | . xtra_tfms | NoneType | None | . size | NoneType | None | . mode | str | bilinear | . pad_mode | str | reflection | . align_corners | bool | True | . batch | bool | False | . min_scale | float | 1.0 | . Show in docs . Next Important class is ImageDataLoaders. . ImageDataLoaders - Wrapper around the DataLoaders with factory methods for computer visions. . In simpler terms and ImageDataLoaders have several helper functions which can easily load the data as a DataLoader object. . According to the docs of fast.ai ImageDataLoader Doc . This class should not be used directly, one of the factory methods should be preferred instead. All those factory methods accept as arguments: . item_tfms: one or several transforms applied to the items before batching them | batch_tfms: one or several transforms applied to the batches once they are formed | bs: the batch size | val_bs: the batch size for the validation DataLoader (defaults to bs) | shuffle_train: if we shuffle the training DataLoader or not device: the PyTorch device to use (defaults to default_device()) | doc(ImageDataLoaders) . class ImageDataLoaders[source] . ImageDataLoaders(*loaders, path=&#39;.&#39;, device=None) :: DataLoaders . Basic wrapper around several DataLoaders with factory methods for computer vision problems . Type Default . loaders | | | . path | str | . | . device | NoneType | None | . Show in docs . tfms = aug_transforms(do_flip=True,flip_vert=True) data= ImageDataLoaders.from_folder(path, train = &quot;train&quot;, valid = &quot;valid&quot;, batch_tfms=[*tfms, Normalize.from_stats(*imagenet_stats)],bs = 16) . Important Note from Transformations perspective . Normalize.from_stats(*imagenet_state): Here we are saying that normalize each image with respect to imagenet_stats (imagenets dimension across 3 channels), basically this gives us the mean and standard deviation tensors of 3 dimensions. | Idea is to bring each pixel value close to the center, so that data dimensions are of approximately the same scale. We&#39;d like in this process for each feature to have a similar range so that our gradients don&#39;t go out of control (and that we only need one global learning rate multiplier). . For example in case of RGB channels, we will do this process for each of the channel by simple formula. Here is a demonstration of how to perform normalization with python using numpy library. . X /= np.std(X, axis = 0) . . image source - https://cs231n.github.io/neural-networks-2/ . The batch size bs is how many images you&#39;ll train at a time. Similarly, we can specify the valid batch size, which defaults to bs we have provided. Smaller bs will work for computers with less memory. . You can use aug_transforms() function to augment your data. I&#39;ll compare the results from flipping images horizontally and vertically. . print(data.vocab) . [&#39;cardboard&#39;, &#39;glass&#39;, &#39;metal&#39;, &#39;paper&#39;, &#39;plastic&#39;, &#39;trash&#39;] . show_batch function fast.ai . A sample image can be seen from data.show_batch method . Takes into argument figure size tuple . This function display the batches of images to quickly glance at the data we are playing around with. . Some most common used arguments . rows: To specify number of rows we want to display in our batches of images | type(data) . fastai.data.core.DataLoaders . data.show_batch(figsize=(10,8)) . Building our Waste Classifier Model . If you run the program with CUDA_LAUNCH_BLOCKING=1, this will help get a more exact stack trace . os.environ[&#39;CUDA_LAUNCH_BLOCKING&#39;] = &quot;1&quot; . learn = cnn_learner(data,models.resnet34,metrics=error_rate) . Cite - Notebook link . What is resnet34? . A residual neural network is a convolutional neural network (CNN) with lots of layers. In particular, resnet34 is a CNN with 34 layers that&#39;s been pretrained on the ImageNet database. A pretrained CNN will perform better on new image classification tasks because it has already learned some visual features and can transfer that knowledge over (hence transfer learning). . Since they&#39;re capable of describing more complexity, deep neural networks should theoretically perform better than shallow networks on training data. In reality, though, deep neural networks tend to perform empirically worse than shallow ones. . Resnets were created to circumvent this glitch using a hack called shortcut connections. If some nodes in a layer have suboptimal values, you can adjust weights and bias; if a node is optimal (its residual is 0), why not leave it alone? Adjustments are only made to nodes on an as-needed basis (when there&#39;s non-zero residuals). . When adjustments are needed, shortcut connections apply the identity function to pass information to subsequent layers. This shortens the neural network when possible and allows resnets to have deep architectures and behave more like shallow neural networks. The 34 in resnet34 just refers to the number of layers. . Here is an interesting links for RESNET architecture . https://blog.roboflow.com/custom-resnet34-classification-model/ . doc(learn.model) . 5, inplace=False) (8): Linear(in_features=512, out_features=6, bias=False) ) )[source] . 5, inplace=False) (8): Linear(in_features=512, out_features=6, bias=False) ) )(*input, **kwargs) . A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then &quot;chains&quot; outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. . The value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential). . What&#39;s the difference between a Sequential and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like--a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way. . Example:: . # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ (&#39;conv1&#39;, nn.Conv2d(1,20,5)), (&#39;relu1&#39;, nn.ReLU()), (&#39;conv2&#39;, nn.Conv2d(20,64,5)), (&#39;relu2&#39;, nn.ReLU()) ])) . Viewing Transformations in action for one image . tr_glass_imgs = os.listdir(str(path)+&#39;/train/glass&#39;)[0] ig=PILImage(PILImage.create(str(path)+&#39;/train/glass/&#39;+tr_glass_imgs).resize((600,400))) . type(array(ig)) . numpy.ndarray . sample_tensor = torch.randn(5,4) # next step is to use permute method after_permute_operation = sample_tensor.permute(1, 0) # permute(1, 0) -&gt; this is essentially swapping up the two axis print(after_permute_operation.shape) assert after_permute_operation.shape[0] == 4 . torch.Size([4, 5]) . array(ig).shape . (400, 600, 3) . # permute method - Used to reorder or reorganize the dimensions of an image timg = TensorImage(array(ig)).permute(2,0,1).float()/255. # Below function expands the dimension to the new batch size to an existing image shape. def _batch_ex(bs): return TensorImage(timg[None].expand(bs, *timg.shape).clone()) . tfms = aug_transforms(do_flip=True) for i in tfms: # Tfms which is a transform object takes into account two class when we pass an argument, do_flip=True print(&quot;Class ===&gt;&gt;&gt;&gt;&quot;, i, i.__getattribute__) # We can now create an object of all the transformations class and then pass our tensor form of images, shown in below code... . Class ===&gt;&gt;&gt;&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5}: encodes: (TensorImage,object) -&gt; encodes (TensorMask,object) -&gt; encodes (TensorBBox,object) -&gt; encodes (TensorPoint,object) -&gt; encodes decodes: &lt;method-wrapper &#39;__getattribute__&#39; of Flip object at 0x7ff627724d10&gt; Class ===&gt;&gt;&gt;&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False}: encodes: (TensorImage,object) -&gt; encodes decodes: &lt;method-wrapper &#39;__getattribute__&#39; of Brightness object at 0x7ff627724850&gt; . y = _batch_ex(2) for t in tfms: # split_idx = 0 refers to we are passing train image.. split_idx = 1, refers to validation in fastai y = t(y, split_idx=0) _,axs = plt.subplots(1,2, figsize=(10,8)) for i,ax in enumerate(axs.flatten()): show_image(y[i], ctx=ax) . Choosing a Learning Rate . Fast Ai uses one cycle policy which simply means, that learning rate will first start with low value, bouncing back to the large values and then being stable with value lower than the initial LR value, this has shown impressive results as it means we are not ending with either a slow or a very high LR. . To choose the best LR, one must look at the point where loss curve is the steepest. Please note that steepest point doesn&#39;t mean the point of minimum loss. It means point where loss is dropping faster.. . . image source - https://iconof.com/1cycle-learning-rate-policy/ . # end_lr = maximum learning rate at which we want the model to stop finding LR. learn.lr_find(start_lr=1e-6,end_lr=1e1) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . SuggestedLRs(valley=0.0012022644514217973) . lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep)) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . import numpy as np np.random.uniform(1e-4, 1e-3) . 0.0007015538560113359 . # One epoch - complete cycle of forward propagation/back propagation learn.fit_one_cycle(20, lr_max=5e-03) . epoch train_loss valid_loss error_rate time . 0 | 1.626455 | 0.673134 | 0.244833 | 01:40 | . 1 | 0.984484 | 0.516873 | 0.189189 | 01:40 | . 2 | 0.834137 | 0.616733 | 0.178060 | 01:40 | . 3 | 0.862364 | 0.832932 | 0.243243 | 01:40 | . 4 | 0.840459 | 0.745179 | 0.225755 | 01:40 | . 5 | 0.778823 | 0.542830 | 0.181240 | 01:40 | . 6 | 0.682324 | 0.816168 | 0.244833 | 01:40 | . 7 | 0.638330 | 0.502149 | 0.155803 | 01:40 | . 8 | 0.557140 | 0.393592 | 0.117647 | 01:40 | . 9 | 0.572882 | 0.529694 | 0.165342 | 01:40 | . 10 | 0.462756 | 0.340510 | 0.109698 | 01:40 | . 11 | 0.385636 | 0.312294 | 0.112878 | 01:40 | . 12 | 0.369283 | 0.339354 | 0.114467 | 01:40 | . 13 | 0.298273 | 0.284633 | 0.084261 | 01:40 | . 14 | 0.274431 | 0.237938 | 0.076312 | 01:40 | . 15 | 0.236636 | 0.229560 | 0.071542 | 01:40 | . 16 | 0.165803 | 0.201756 | 0.068362 | 01:40 | . 17 | 0.143642 | 0.204025 | 0.069952 | 01:40 | . 18 | 0.144287 | 0.192077 | 0.066773 | 01:40 | . 19 | 0.185571 | 0.196076 | 0.071542 | 01:39 | . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . The model ran for 20 epochs giving us the minimum loss of 0.066 which is better than the previous model, which was around 0.08. Hence, we are able to reduce the loss and increase the accuracy which will see later. . VIsualizing most incorrect images . interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . plt.figure(figsize=(10, 8)) interp.plot_top_losses(4, nrows=2) plt.show() . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . &lt;Figure size 720x576 with 0 Axes&gt; . interp.plot_top_losses(10, figsize=(15,11)) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . The images here are the ones after the removal of some over exposure of light in images, which was not the case with previous notebook of collindching. This has created an impact as now our model is less confused and hence the loss decreased. . doc(interp.plot_top_losses) . Interpretation.plot_top_losses[source] . Interpretation.plot_top_losses(k, largest=True, **kwargs) . Show k largest(/smallest) preds and losses. k may be int, list, or range of desired results. . Type Default . k | | | . largest | bool | True | . kwargs | | . Show in docs . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . In the previous version of this notebook by collindching, The model often confused plastic for glass and confused metal for glass. The list of most confused images is below. Let&#39;s see are we able to reduce the overall misclassification error for the categories or not later. . interp.most_confused(min_val=2) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . [(&#39;glass&#39;, &#39;metal&#39;, 7), (&#39;cardboard&#39;, &#39;paper&#39;, 5), (&#39;glass&#39;, &#39;plastic&#39;, 5), (&#39;plastic&#39;, &#39;metal&#39;, 4), (&#39;plastic&#39;, &#39;paper&#39;, 4), (&#39;trash&#39;, &#39;paper&#39;, 4), (&#39;metal&#39;, &#39;paper&#39;, 2), (&#39;metal&#39;, &#39;plastic&#39;, 2), (&#39;plastic&#39;, &#39;trash&#39;, 2)] . 4. Predicting the test data . To see how this mode really performs, we need to make predictions on test data. First, I&#39;ll make predictions on the test data using the learner.get_preds() method. . Note: learner.predict() only predicts on a single image, while learner.get_preds() predicts on a set of images. I highly recommend reading the documentation to learn more about predict() and get_preds(). . doc(learn.predict) . Learner.predict[source] . Learner.predict(item, rm_type_tfms=None, with_input=False) . Prediction on item, fully decoded, loss function decoded and probabilities . Type Default . item | | | . rm_type_tfms | NoneType | None | . with_input | bool | False | . Show in docs . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . Loading Test DataLoader for Prediction . In the above cells, we have defined data loader for our train and validation set.. . However, for our model to predict on the new image data, we will have load into a dataloader object. . We can simply pass the images of test folder into get_image_files function, which will traverse the test folder and get us all the images in test folder. . get_preds is the function which gives us the probabilities of our image being of each class . test_dl = data.test_dl(get_image_files(os.path.join(path, &#39;test&#39;))) preds = learn.get_preds(dl=test_dl) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . print(preds[0].shape) preds[0] . torch.Size([631, 6]) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . TensorBase([[1.7182e-04, 7.1475e-05, 1.5689e-03, 2.3538e-05, 9.9789e-01, 2.7502e-04], [8.1237e-08, 5.9949e-08, 1.6182e-08, 9.9999e-01, 1.9660e-07, 1.1921e-05], [1.3310e-06, 6.2397e-03, 9.9346e-01, 4.1719e-07, 2.6252e-04, 3.6827e-05], ..., [5.1307e-06, 5.3394e-05, 9.9495e-01, 1.8174e-04, 9.8943e-05, 4.7127e-03], [1.7216e-07, 1.0502e-05, 7.9543e-04, 7.6339e-06, 9.9208e-05, 9.9909e-01], [8.3721e-06, 5.6436e-08, 6.0167e-08, 9.9991e-01, 1.5351e-05, 6.4679e-05]]) . Converting probabilities to a class names . Simple approach - For one set of image, get the maximum probability value among 6 classes. . Choose the one with maximum value along the 1 axis i.e columns . Rows - Probabilities for other sample of images . max_idxs = np.asarray(np.argmax(preds[0],axis=1)) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . max_idxs = np.asarray(np.argmax(preds[0],axis=1)) classes = data.vocab print(classes) yhat = [] for max_idx in max_idxs: yhat.append(classes[max_idx]) . [&#39;cardboard&#39;, &#39;glass&#39;, &#39;metal&#39;, &#39;paper&#39;, &#39;plastic&#39;, &#39;trash&#39;] . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . l = get_image_files(os.path.join(path, &#39;test&#39;)) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . from PIL import Image . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . Image.open(l[0]) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . learn.validate(dl=test_dl) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . (#2) [None,None] . y = [] ## convert POSIX paths to string first for label_path in test_dl.items: y.append(str(label_path)) # then extract waste type from file path pattern = re.compile(&quot;([a-z]+)[0-9]+&quot;) for i in range(len(y)): y[i] = pattern.search(y[i]).group(1) . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . Creating a Dataframe of the results observed . print(yhat[0:30]) ## actual values print(y[0:30]) df_dict = {&#39;actual&#39;: y, &#39;predicted&#39;: yhat} pd.DataFrame(df_dict) . [&#39;plastic&#39;, &#39;paper&#39;, &#39;metal&#39;, &#39;paper&#39;, &#39;cardboard&#39;, &#39;cardboard&#39;, &#39;paper&#39;, &#39;trash&#39;, &#39;cardboard&#39;, &#39;glass&#39;, &#39;plastic&#39;, &#39;paper&#39;, &#39;paper&#39;, &#39;cardboard&#39;, &#39;glass&#39;, &#39;plastic&#39;, &#39;glass&#39;, &#39;plastic&#39;, &#39;plastic&#39;, &#39;glass&#39;, &#39;paper&#39;, &#39;glass&#39;, &#39;paper&#39;, &#39;trash&#39;, &#39;cardboard&#39;, &#39;paper&#39;, &#39;plastic&#39;, &#39;metal&#39;, &#39;paper&#39;, &#39;paper&#39;] [&#39;plastic&#39;, &#39;paper&#39;, &#39;metal&#39;, &#39;paper&#39;, &#39;cardboard&#39;, &#39;cardboard&#39;, &#39;paper&#39;, &#39;trash&#39;, &#39;cardboard&#39;, &#39;glass&#39;, &#39;plastic&#39;, &#39;plastic&#39;, &#39;paper&#39;, &#39;cardboard&#39;, &#39;glass&#39;, &#39;plastic&#39;, &#39;glass&#39;, &#39;plastic&#39;, &#39;plastic&#39;, &#39;glass&#39;, &#39;paper&#39;, &#39;glass&#39;, &#39;paper&#39;, &#39;trash&#39;, &#39;cardboard&#39;, &#39;paper&#39;, &#39;plastic&#39;, &#39;metal&#39;, &#39;paper&#39;, &#39;paper&#39;] . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . actual predicted . 0 plastic | plastic | . 1 paper | paper | . 2 metal | metal | . 3 paper | paper | . 4 cardboard | cardboard | . ... ... | ... | . 626 plastic | plastic | . 627 glass | glass | . 628 metal | metal | . 629 trash | trash | . 630 paper | paper | . 631 rows × 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; It looks the first five predictions match up! (check) . How did we end up doing? Again we can use a confusion matrix to find out. . Test confusion matrix . cm = confusion_matrix(y,yhat) print(cm) . [[ 96 0 2 2 1 0] [ 0 111 5 0 6 0] [ 0 5 98 0 0 0] [ 1 0 0 145 1 2] [ 0 2 0 1 117 1] [ 0 0 0 5 3 27]] . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . Visualising Confusion Matrix of Test Set . df_cm = pd.DataFrame(cm,waste_types,waste_types) plt.figure(figsize=(10,8)) sns.heatmap(df_cm,annot=True,fmt=&quot;d&quot;,cmap=&quot;YlGnBu&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f084460cdd0&gt; . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . correct = 0 for r in range(len(cm)): for c in range(len(cm)): if (r==c): correct += cm[r,c] . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . accuracy = correct/sum(sum(cm)) accuracy . 0.9413629160063391 . /usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default. return _iterencode(o, 0) . We ended up achieving 94.1% accuracy which is slightly better than previous notebook. Also, we were able to work on the next steps mentioned by collindching&#39;s notebook and reduce misclassification error as well. . Comparing our confusion matrix with previous confusion matrix . Collindching&#39;s version of CM Vs My version . . My Version . Google Collaboratory Link for the code and experiments . https://github.com/amay1212/Waste-Sorting/blob/master/Waste_Sorter%20Extended.ipynb . Further enhancements and Research area . To improve accuracy even further for the misclassified results. . | To try and test this model with back rep using the below . https://www.mdpi.com/2313-433X/7/8/144/pdf#:~:text=BackRep%20consists%20of%20a%20data,solid%20waste%20is%20usually%20littered. . | To see if we can distinguish more clearly between dry and wet waste in particular. . |",
            "url": "https://amay-trivedi.github.io/blog-with-fast-ai/2022/02/20/Waste-Sorting-Extended-Version-Fast-AI-v2.html",
            "relUrl": "/2022/02/20/Waste-Sorting-Extended-Version-Fast-AI-v2.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://amay-trivedi.github.io/blog-with-fast-ai/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://amay-trivedi.github.io/blog-with-fast-ai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Amay Trivedi and I am currently working as a Machine Learning/Deep learning professional. . I got deeply interested about Jeremy’s notes being my forefront go-to. Currently, exploring the Practical Deep Learning course for coders Part 2020 and learning new ideas on the go. I try to implement the problems in different fields for this domain and curate my own dataset. . Contact me . amaytrivedi12995@gmail.com .",
          "url": "https://amay-trivedi.github.io/blog-with-fast-ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://amay-trivedi.github.io/blog-with-fast-ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}